{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying corrections to columnar data\n",
    "\n",
    "Here we will show how to use the lookup_tools package in fnal-column-analysis-tools (FCAT).\n",
    "It is able to read in a variety of correction files as lookup tables.\n",
    "We also cover here the CMS-specific JET-MET tools that are provided with FCAT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.util import awkward\n",
    "from coffea.util import numpy as np\n",
    "import uproot_methods\n",
    "\n",
    "\n",
    "def dummy_jagged_eta_pt():\n",
    "    np.random.seed(42)\n",
    "    counts = np.random.exponential(2, size=50).astype(int)\n",
    "    entries = np.sum(counts)\n",
    "    test_eta = np.random.uniform(-3., 3., size=entries)\n",
    "    test_pt = np.random.exponential(10., size=entries)+np.random.exponential(10, size=entries)\n",
    "    return (counts, test_eta, test_pt)\n",
    "\n",
    "def dummy_four_momenta():\n",
    "    np.random.seed(12345)\n",
    "    nrows = 1000\n",
    "    counts = np.minimum(np.random.exponential(0.5, size=nrows).astype(int), 20)\n",
    "    \n",
    "    px = np.random.normal(loc=20.0,scale=5.0,size=np.sum(counts))\n",
    "    py = np.random.normal(loc=20.0,scale=5.0,size=np.sum(counts))\n",
    "    pz = np.random.normal(loc=0, scale=55, size=np.sum(counts))\n",
    "    m_pi = np.full_like(px,fill_value=0.135)\n",
    "    energy = np.sqrt(px*px + py*py + pz*pz + m_pi*m_pi)\n",
    "    return (counts,px,py,pz,energy)\n",
    "\n",
    "def dummy_events():\n",
    "    counts, px, py, pz, energy = dummy_four_momenta()\n",
    "    thep4 = np.stack((px,py,pz,energy)).T\n",
    "\n",
    "    class obj(object):\n",
    "        def __init__(self):\n",
    "            self.p4 = thep4\n",
    "            self.px = px\n",
    "            self.py = py\n",
    "            self.pz = pz\n",
    "            self.en = energy\n",
    "            self.pt = np.hypot(px,py)\n",
    "            self.phi = np.arctan2(py,px)\n",
    "            self.eta = np.arctanh(pz/np.sqrt(px*px + py*py + pz*pz))\n",
    "            self.mass = np.sqrt(np.abs(energy*energy - (px*px + py*py + pz*pz)))\n",
    "            self.blah = energy*px\n",
    "            self.count = counts\n",
    "    \n",
    "    class events(object):\n",
    "        def __init__(self):\n",
    "            self.thing = obj()\n",
    "    \n",
    "    return events()\n",
    "\n",
    "def gen_reco_TLV():\n",
    "    gen_pt = awkward.JaggedArray.fromiter([[10.0, 20.0, 30.0], [], [40.0, 50.0]])\n",
    "    reco_pt = awkward.JaggedArray.fromiter([[20.2, 10.1, 30.3, 50.5], [50.5], [60]])\n",
    "\n",
    "    gen_eta = awkward.JaggedArray.fromiter([[-3.0, -2.0, 2.0], [], [-1.0, 1.0]])\n",
    "    reco_eta = awkward.JaggedArray.fromiter([[-2.2, -3.3, 2.2, 0.0], [0.0], [1.1]])\n",
    "\n",
    "    gen_phi = awkward.JaggedArray.fromiter([[-1.5, 0.0, 1.5], [], [0.78, -0.78]])\n",
    "    reco_phi = awkward.JaggedArray.fromiter([[ 0.1, -1.4, 1.4, 0.78], [0.78], [-0.77]])\n",
    "\n",
    "    gen = uproot_methods.TLorentzVectorArray.from_ptetaphim(gen_pt, gen_eta, gen_phi, 0.2)\n",
    "    reco = uproot_methods.TLorentzVectorArray.from_ptetaphim(reco_pt, reco_eta, reco_phi, 0.2)\n",
    "\n",
    "    return (gen, reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.lookup_tools import extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening a root file and using it as a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = extractor()\n",
    "ext.add_weight_sets([\"testSF2d scalefactors_Tight_Electron ../tests/samples/testSF2d.histo.root\"])\n",
    "ext.finalize()\n",
    "\n",
    "evaluator = ext.make_evaluator()\n",
    "\n",
    "print(dir(evaluator))\n",
    "print()\n",
    "print(evaluator['testSF2d'])\n",
    "print()\n",
    "print(type(evaluator['testSF2d']))\n",
    "print()\n",
    "\n",
    "counts, test_eta, test_pt = dummy_jagged_eta_pt()\n",
    "\n",
    "# test flat eval\n",
    "test_out = evaluator[\"testSF2d\"](test_eta, test_pt)\n",
    "\n",
    "# test structured eval\n",
    "test_eta_jagged = awkward.JaggedArray.fromcounts(counts, test_eta)\n",
    "test_pt_jagged = awkward.JaggedArray.fromcounts(counts, test_pt)\n",
    "test_out_jagged = evaluator[\"testSF2d\"](test_eta_jagged, test_pt_jagged)\n",
    "\n",
    "print('JAGGED SHAPE COMPARISON')\n",
    "print('eta      :',test_eta_jagged.counts)\n",
    "print('pt     :',test_pt_jagged.counts)\n",
    "print('scaling :',test_out_jagged.counts)\n",
    "\n",
    "print('output scaling :',test_out_jagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import hist\n",
    "\n",
    "weights_axis = hist.Cat(\"wgtname\", \"With/Without Weights\")\n",
    "eta_axis = hist.Bin(\"eta\", r\"$\\eta\", 20, -2.5, 2.5)\n",
    "pt_axis = hist.Bin(\"pt\", r\"$p_{T}$ [GeV]\", 20, 10, 30)\n",
    "\n",
    "eta_hist = hist.Hist(\"Counts\", weights_axis, eta_axis)\n",
    "pt_hist = hist.Hist(\"Counts\", weights_axis, pt_axis)\n",
    "\n",
    "eta_hist.fill(wgtname='weights',eta=test_eta_jagged.flatten(),weight=test_out_jagged.flatten())\n",
    "eta_hist.fill(wgtname='noweights',eta=test_eta_jagged.flatten())\n",
    "\n",
    "pt_hist.fill(wgtname='weights',pt=test_pt_jagged.flatten(),weight=test_out_jagged.flatten())\n",
    "pt_hist.fill(wgtname='noweights',pt=test_pt_jagged.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, _ = hist.plot1d(eta_hist, overlay='wgtname')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, _ = hist.plot1d(pt_hist, overlay='wgtname')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing B-tagging Scale Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -5 ../tests/samples/testBTagSF.btag.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = extractor()\n",
    "ext.add_weight_sets([\"testBTag * ../tests/samples/testBTagSF.btag.csv\"])\n",
    "ext.finalize()\n",
    "\n",
    "evaluator = ext.make_evaluator()\n",
    "\n",
    "print(dir(evaluator))\n",
    "print()\n",
    "print(evaluator['testBTagCSVv2_1_comb_up_0'])\n",
    "print()\n",
    "print(type(evaluator['testBTagCSVv2_1_comb_up_0']))\n",
    "print()\n",
    "\n",
    "counts, test_eta, test_pt = dummy_jagged_eta_pt()\n",
    "# discriminant used for reshaping, zero otherwise\n",
    "test_discr = np.zeros_like(test_eta)\n",
    "\n",
    "sf_out = evaluator['testBTagCSVv2_1_comb_up_0'](test_eta, test_pt, test_discr)\n",
    "print(sf_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing JSON-encoded histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -10 ../tests/samples/EIDISO_WH_out.histo.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = extractor()\n",
    "ext.add_weight_sets([\"* * ../tests/samples/EIDISO_WH_out.histo.json\"])\n",
    "ext.finalize()\n",
    "    \n",
    "evaluator = ext.make_evaluator()\n",
    "\n",
    "print(dir(evaluator))\n",
    "print()\n",
    "print(evaluator['EIDISO_WH/eta_pt_ratio_value'])\n",
    "print()\n",
    "print(type(evaluator['EIDISO_WH/eta_pt_ratio_value']))\n",
    "print()\n",
    "\n",
    "counts, test_eta, test_pt = dummy_jagged_eta_pt()\n",
    "    \n",
    "sf_out = evaluator['EIDISO_WH/eta_pt_ratio_value'](test_eta, test_pt)\n",
    "sf_err_out = evaluator['EIDISO_WH/eta_pt_ratio_error'](test_eta, test_pt)\n",
    "print(sf_out)\n",
    "print(sf_err_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Jet Energy Scales and Uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -5 ../tests/samples/Fall17_17Nov2017_V32_MC_L2Relative_AK4PFPuppi.jec.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = extractor()\n",
    "ext.add_weight_sets([\n",
    "    \"* * ../tests/samples/Fall17_17Nov2017_V32_MC_L2Relative_AK4PFPuppi.jec.txt\",\n",
    "    \"* * ../tests/samples/Fall17_17Nov2017_V32_MC_Uncertainty_AK4PFPuppi.junc.txt\",\n",
    "])\n",
    "ext.finalize()\n",
    "\n",
    "evaluator = ext.make_evaluator()\n",
    "\n",
    "print(dir(evaluator))\n",
    "print()\n",
    "print(evaluator['Fall17_17Nov2017_V32_MC_L2Relative_AK4PFPuppi'])\n",
    "print()\n",
    "print(evaluator['Fall17_17Nov2017_V32_MC_Uncertainty_AK4PFPuppi'])\n",
    "print()\n",
    "print(type(evaluator['Fall17_17Nov2017_V32_MC_L2Relative_AK4PFPuppi']))\n",
    "print()\n",
    "print(type(evaluator['Fall17_17Nov2017_V32_MC_Uncertainty_AK4PFPuppi']))\n",
    "print()\n",
    "\n",
    "counts, test_eta, test_pt = dummy_jagged_eta_pt()\n",
    "    \n",
    "jec_out = evaluator['Fall17_17Nov2017_V32_MC_L2Relative_AK4PFPuppi'](test_eta,test_pt)\n",
    "\n",
    "print(jec_out)\n",
    "\n",
    "junc_out = evaluator['Fall17_17Nov2017_V32_MC_Uncertainty_AK4PFPuppi'](test_eta,test_pt)\n",
    "\n",
    "print(junc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -4 ../tests/samples/Autumn18_V8_MC_UncertaintySources_AK4PFchs.junc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = extractor()\n",
    "ext.add_weight_sets([\n",
    "    \"* * ../tests/samples/Autumn18_V8_MC_UncertaintySources_AK4PFchs.junc.txt\",\n",
    "])\n",
    "ext.finalize()\n",
    "\n",
    "evaluator = ext.make_evaluator()\n",
    "\n",
    "print(dir(evaluator))\n",
    "print()\n",
    "print(evaluator['Autumn18_V8_MC_UncertaintySources_AK4PFchs_AbsoluteScale'])\n",
    "print()\n",
    "print(type(evaluator['Autumn18_V8_MC_UncertaintySources_AK4PFchs_AbsoluteScale']))\n",
    "print()\n",
    "\n",
    "bysource_junc_out = evaluator['Autumn18_V8_MC_UncertaintySources_AK4PFchs_AbsoluteScale'](test_eta,test_pt)\n",
    "print(bysource_junc_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying energy scale transformations to Jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.analysis_objects import JaggedCandidateArray as CandArray\n",
    "from coffea.jetmet_tools import FactorizedJetCorrector,JetCorrectionUncertainty\n",
    "from coffea.jetmet_tools import JetTransformer\n",
    "\n",
    "ext = extractor()\n",
    "ext.add_weight_sets([\n",
    "    \"* * ../tests/samples/Fall17_17Nov2017_V32_MC_L2Relative_AK4PFPuppi.jec.txt\",\n",
    "    \"* * ../tests/samples/Fall17_17Nov2017_V32_MC_Uncertainty_AK4PFPuppi.junc.txt\",\n",
    "])\n",
    "ext.finalize()\n",
    "\n",
    "evaluator = ext.make_evaluator()\n",
    "\n",
    "print(dir(evaluator))\n",
    "print()\n",
    "\n",
    "counts, px, py, pz, E = dummy_four_momenta()\n",
    "\n",
    "jets = CandArray.candidatesfromcounts(counts,px=px,py=py,pz=pz,energy=E)\n",
    "jets.add_attributes(ptRaw=jets.pt,\n",
    "                    massRaw=jets.mass)\n",
    "\n",
    "jec_names = ['Fall17_17Nov2017_V32_MC_L2Relative_AK4PFPuppi']\n",
    "junc_names = ['Fall17_17Nov2017_V32_MC_Uncertainty_AK4PFPuppi']\n",
    "corrector = FactorizedJetCorrector(**{name: evaluator[name] for name in jec_names})\n",
    "uncertainties = JetCorrectionUncertainty(**{name:evaluator[name] for name in junc_names})\n",
    "\n",
    "transformer = JetTransformer(jec=corrector,junc=uncertainties)\n",
    "### more possibilities are available if you send in more pieces of the JEC stack\n",
    "# mc2016_ak8_jxform = JetTransformer(jec=MC_AK8JEC2016,junc=MC_AK8JUNC2016\n",
    "#                                    jer=MC_AK8JER2016,jersf=MC_AK8JERSF2016)\n",
    "\n",
    "print()\n",
    "print('starting columns:',jets.columns)\n",
    "print()\n",
    "\n",
    "print('untransformed pt ratios',jets.pt/jets.ptRaw)\n",
    "print('untransformed mass ratios',jets.mass/jets.massRaw)\n",
    "\n",
    "transformer.transform(jets)\n",
    "\n",
    "print('transformed pt ratios',jets.pt/jets.ptRaw)\n",
    "print('transformed mass ratios',jets.mass/jets.massRaw)\n",
    "\n",
    "print()\n",
    "print('transformed columns:',jets.columns)\n",
    "print()\n",
    "\n",
    "print('JES UP pt ratio',jets.pt_AK4PFPuppi_up/jets.ptRaw)\n",
    "print('JET DOWN pt ratio',jets.pt_AK4PFPuppi_down/jets.ptRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
