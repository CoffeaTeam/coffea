{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffea Processors\n",
    "This is a rendered copy of [processor.ipynb](https://github.com/CoffeaTeam/coffea/blob/master/binder/processor.ipynb). You can optionally run it interactively on [binder at this link](https://mybinder.org/v2/gh/coffeateam/coffea/master?filepath=binder%2Fprocessor.ipynb)\n",
    "\n",
    "Coffea relies mainly on [uproot](https://github.com/scikit-hep/uproot) to provide access to ROOT files for analysis.\n",
    "As a usual analysis will involve processing tens to thousands of files, totalling gigabytes to terabytes of data, there is a certain amount of work to be done to build a parallelized framework to process the data in a reasonable amount of time. Of course, one can work directly within uproot to achieve this, as we'll show in the beginning, but coffea provides the `coffea.processor` module, which allows users to worry just about the actual analysis code and not about how to implement efficient parallelization, assuming that the parallization is a trivial map-reduce operation (e.g. filling histograms and adding them together). The module provides the following key features:\n",
    "\n",
    " * A `ProcessorABC` abstract base class that can be derived from to implement the analysis code;\n",
    " * An interface to the arrays being read from the TTree, either [DataFrame](https://coffeateam.github.io/coffea/api/coffea.processor.LazyDataFrame.html#coffea.processor.LazyDataFrame) or [NanoEvents](https://coffeateam.github.io/coffea/notebooks/nanoevents.html), to be used as inputs;\n",
    " * A set of [accumulator types](https://coffeateam.github.io/coffea/api/coffea.processor.AccumulatorABC.html#coffea.processor.AccumulatorABC), such as `dict_accumulator` or `Hist` to be used as output; and\n",
    " * A set of parallel executors to access multicore processing or distributed computing systems such as [Dask](https://distributed.dask.org/en/latest/), [Parsl](http://parsl-project.org/), [Spark](https://spark.apache.org/), and others.\n",
    "\n",
    "Let's start by writing a simple processor class that reads some CMS open data and plots a dimuon mass spectrum.\n",
    "We'll start by copying the [ProcessorABC](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html#coffea.processor.ProcessorABC) skeleton and filling in some details:\n",
    "\n",
    " * Remove `flag`, as we won't use it\n",
    " * Adding a new histogram for $m_{\\mu \\mu}$\n",
    " * Building a [Candidate](https://coffeateam.github.io/coffea/api/coffea.nanoevents.methods.candidate.PtEtaPhiMCandidate.html#coffea.nanoevents.methods.candidate.PtEtaPhiMCandidate) record for muons\n",
    " * Calculating the dimuon invariant mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "from coffea import processor\n",
    "from hist import Hist\n",
    "\n",
    "# register our candidate behaviors\n",
    "from coffea.nanoevents.methods import candidate\n",
    "ak.behavior.update(candidate.behavior)\n",
    "\n",
    "class MyProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def process(self, events):\n",
    "        h_mass = (\n",
    "            Hist.new\n",
    "            .StrCategory([], name=\"dataset\", growth=True)\n",
    "            .Reg(60, 60, 120, name=\"mass\", label=\"Z mass\")\n",
    "            .Int64()\n",
    "        )\n",
    "\n",
    "        dataset = events.metadata['dataset']\n",
    "        muons = ak.zip({\n",
    "            \"pt\": events.Muon_pt,\n",
    "            \"eta\": events.Muon_eta,\n",
    "            \"phi\": events.Muon_phi,\n",
    "            \"mass\": events.Muon_mass,\n",
    "            \"charge\": events.Muon_charge,\n",
    "        }, with_name=\"PtEtaPhiMCandidate\")\n",
    "\n",
    "        cut = (ak.num(muons) == 2) & (ak.sum(muons.charge) == 0)\n",
    "        # add first and second muon in every event together\n",
    "        dimuon = muons[cut][:, 0] + muons[cut][:, 1]\n",
    "        \n",
    "        h_mass.fill(\n",
    "            dataset=dataset,\n",
    "            mass=dimuon.mass,\n",
    "        )\n",
    "\n",
    "        return { \n",
    "            \"sumw\": {dataset: float(len(events))},\n",
    "            \"mass\": h_mass,\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to just use bare uproot to execute this processor, we could do that with the following example, which:\n",
    "\n",
    " * Opens a CMS open data file\n",
    " * Creates a NanoEvents object using `BaseSchema` (roughly equivalent to the output of `uproot.lazy`)\n",
    " * Creates a `MyProcessor` instance\n",
    " * Runs the `process()` function, which returns our accumulators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "from coffea.nanoevents import NanoEventsFactory, BaseSchema\n",
    "\n",
    "# https://github.com/scikit-hep/uproot4/issues/122\n",
    "uproot.open.defaults[\"xrootd_handler\"] = uproot.source.xrootd.MultithreadedXRootDSource\n",
    "\n",
    "filename = \"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root\"\n",
    "file = uproot.open(filename)\n",
    "events = NanoEventsFactory.from_root(\n",
    "    file,\n",
    "    entry_stop=10000,\n",
    "    metadata={\"dataset\": \"DoubleMuon\"},\n",
    "    schemaclass=BaseSchema,\n",
    ").events()\n",
    "p = MyProcessor()\n",
    "out = p.process(events)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could expand on this code to run over several chunks of the file, setting `entry_start` and `entry_stop` as appropriate. Then, several datasets could be processed by iterating over several files. However, [run_uproot_job](https://coffeateam.github.io/coffea/api/coffea.processor.run_uproot_job.html#coffea.processor.run_uproot_job) can help with this. One lists the datasets and corresponding files, the processor they want to run, and which executor they want to use. Available executors are listed [here](https://coffeateam.github.io/coffea/modules/coffea.processor.html#functions). Since these files are very large, we limit to just reading the first few chunks of events from each dataset with `maxchunks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = {\n",
    "    'DoubleMuon': [\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root',\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012C_DoubleMuParked.root',\n",
    "    ],\n",
    "    'ZZ to 4mu': [\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/ZZTo4mu.root'\n",
    "    ]\n",
    "}\n",
    "\n",
    "iterative_run = processor.Runner(\n",
    "    executor = processor.IterativeExecutor(compression=None),\n",
    "    schema=BaseSchema,\n",
    "    maxchunks=4,\n",
    ")\n",
    "\n",
    "out = iterative_run(\n",
    "    fileset,\n",
    "    treename=\"Events\",\n",
    "    processor_instance=MyProcessor(),\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to use more than a single core on our machine, we simply change [iterative_executor](https://coffeateam.github.io/coffea/api/coffea.processor.iterative_executor.html) for [futures_executor](https://coffeateam.github.io/coffea/api/coffea.processor.futures_executor.html), which uses the python [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html) standard library. Optional arguments to these executors can be provided via `executor_args` parameter of `run_uproot_job`, such as the number of cores to use (2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures_run = processor.Runner(\n",
    "    executor = processor.FuturesExecutor(compression=None, workers=2),\n",
    "    schema=BaseSchema,\n",
    "    maxchunks=4,\n",
    ")\n",
    "\n",
    "out = futures_run(\n",
    "    fileset,\n",
    "    \"Events\",\n",
    "    processor_instance=MyProcessor()\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this ran faster than the previous cell, but that may depend on how many cores are available on the machine you are running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting fancy\n",
    "Let's flesh out this analysis into a 4-muon analysis, searching for diboson events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from coffea import hist\n",
    "from functools import partial\n",
    "import numba\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def find_4lep(events_leptons, builder):\n",
    "    \"\"\"Search for valid 4-lepton combinations from an array of events * leptons {charge, ...}\n",
    "    \n",
    "    A valid candidate has two pairs of leptons that each have balanced charge\n",
    "    Outputs an array of events * candidates {indices 0..3} corresponding to all valid\n",
    "    permutations of all valid combinations of unique leptons in each event\n",
    "    (omitting permutations of the pairs)\n",
    "    \"\"\"\n",
    "    for leptons in events_leptons:\n",
    "        builder.begin_list()\n",
    "        nlep = len(leptons)\n",
    "        for i0 in range(nlep):\n",
    "            for i1 in range(i0 + 1, nlep):\n",
    "                if leptons[i0].charge + leptons[i1].charge != 0:\n",
    "                    continue\n",
    "                for i2 in range(nlep):\n",
    "                    for i3 in range(i2 + 1, nlep):\n",
    "                        if len({i0, i1, i2, i3}) < 4:\n",
    "                            continue\n",
    "                        if leptons[i2].charge + leptons[i3].charge != 0:\n",
    "                            continue\n",
    "                        builder.begin_tuple(4)\n",
    "                        builder.index(0).integer(i0)\n",
    "                        builder.index(1).integer(i1)\n",
    "                        builder.index(2).integer(i2)\n",
    "                        builder.index(3).integer(i3)\n",
    "                        builder.end_tuple()\n",
    "        builder.end_list()\n",
    "\n",
    "    return builder\n",
    "\n",
    "\n",
    "class FancyDimuonProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self, events):\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"Primary dataset\")\n",
    "        mass_axis = hist.Bin(\"mass\", r\"$m_{\\mu\\mu}$ [GeV]\", 300, 0, 300)\n",
    "        pt_axis = hist.Bin(\"pt\", r\"$p_{T,\\mu}$ [GeV]\", 300, 0, 300)\n",
    "        \n",
    "        h_nMuons = hist.Hist(\n",
    "            \"Counts\",\n",
    "            dataset_axis,\n",
    "            hist.Bin(\"nMuons\", \"Number of good muons\", 6, 0, 6),\n",
    "        )\n",
    "        h_m4mu = hist.Hist(\"Counts\", dataset_axis, mass_axis)\n",
    "        h_mZ1 = hist.Hist(\"Counts\", dataset_axis, mass_axis)\n",
    "        h_mZ2 = hist.Hist(\"Counts\", dataset_axis, mass_axis)\n",
    "        h_ptZ1mu1 = hist.Hist(\"Counts\", dataset_axis, pt_axis)\n",
    "        h_ptZ1mu2 = hist.Hist(\"Counts\", dataset_axis, pt_axis)\n",
    "                \n",
    "        cutflow = defaultdict(int)\n",
    "        \n",
    "        dataset = events.metadata['dataset']\n",
    "        muons = ak.zip({\n",
    "            \"pt\": events.Muon_pt,\n",
    "            \"eta\": events.Muon_eta,\n",
    "            \"phi\": events.Muon_phi,\n",
    "            \"mass\": events.Muon_mass,\n",
    "            \"charge\": events.Muon_charge,\n",
    "            \"softId\": events.Muon_softId,\n",
    "            \"isolation\": events.Muon_pfRelIso03_all,\n",
    "        }, with_name=\"PtEtaPhiMCandidate\")\n",
    "        \n",
    "        # make sure they are sorted by transverse momentum\n",
    "        muons = muons[ak.argsort(muons.pt, axis=1)]\n",
    "        \n",
    "        cutflow['all events'] += len(muons)\n",
    "        \n",
    "        # impose some quality and minimum pt cuts on the muons\n",
    "        muons = muons[\n",
    "            muons.softId\n",
    "            & (muons.pt > 5)\n",
    "            & (muons.isolation < 0.2)\n",
    "        ]\n",
    "        cutflow['at least 4 good muons'] += ak.sum(ak.num(muons) >= 4)\n",
    "        h_nMuons.fill(dataset=dataset, nMuons=ak.num(muons))\n",
    "        \n",
    "        # reduce first axis: skip events without enough muons\n",
    "        muons = muons[ak.num(muons) >= 4]\n",
    "        \n",
    "        # find all candidates with helper function\n",
    "        fourmuon = find_4lep(muons, ak.ArrayBuilder()).snapshot()\n",
    "        if ak.all(ak.num(fourmuon) == 0):\n",
    "            # skip processing as it is an EmptyArray\n",
    "            return output\n",
    "        fourmuon = [muons[fourmuon[idx]] for idx in \"0123\"]\n",
    "        fourmuon = ak.zip({\n",
    "            \"z1\": ak.zip({\n",
    "                \"lep1\": fourmuon[0],\n",
    "                \"lep2\": fourmuon[1],\n",
    "                \"p4\": fourmuon[0] + fourmuon[1],\n",
    "            }),\n",
    "            \"z2\": ak.zip({\n",
    "                \"lep1\": fourmuon[2],\n",
    "                \"lep2\": fourmuon[3],\n",
    "                \"p4\": fourmuon[2] + fourmuon[3],\n",
    "            }),\n",
    "        })\n",
    "        \n",
    "        cutflow['at least one candidate'] += ak.sum(ak.num(fourmuon) > 0)\n",
    "         \n",
    "        # require minimum dimuon mass\n",
    "        fourmuon = fourmuon[(fourmuon.z1.p4.mass > 60.) & (fourmuon.z2.p4.mass > 20.)]\n",
    "        cutflow['minimum dimuon mass'] += ak.sum(ak.num(fourmuon) > 0)\n",
    "        \n",
    "        # choose permutation with z1 mass closest to nominal Z boson mass\n",
    "        bestz1 = ak.singletons(ak.argmin(abs(fourmuon.z1.p4.mass - 91.1876), axis=1))\n",
    "        fourmuon = ak.flatten(fourmuon[bestz1])\n",
    "        \n",
    "        h_m4mu.fill(\n",
    "            dataset=dataset,\n",
    "            mass=(fourmuon.z1.p4 + fourmuon.z2.p4).mass,\n",
    "        )\n",
    "        h_mZ1.fill(\n",
    "            dataset=dataset, \n",
    "            mass=fourmuon.z1.p4.mass,\n",
    "        )\n",
    "        h_mZ2.fill(\n",
    "            dataset=dataset, \n",
    "            mass=fourmuon.z2.p4.mass,\n",
    "        )\n",
    "        h_ptZ1mu1.fill(\n",
    "            dataset=dataset,\n",
    "            pt=fourmuon.z1.lep1.pt,\n",
    "        )\n",
    "        h_ptZ1mu2.fill(\n",
    "            dataset=dataset,\n",
    "            pt=fourmuon.z1.lep2.pt,\n",
    "        )\n",
    "        return {\n",
    "            'nMuons': h_nMuons,\n",
    "            'mass': h_m4mu,\n",
    "            'mass_z1': h_mZ1,\n",
    "            'mass_z2': h_mZ2,\n",
    "            'pt_z1_mu1': h_ptZ1mu1,\n",
    "            'pt_z1_mu2': h_ptZ1mu2,\n",
    "            'cutflow': {dataset: cutflow},\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tstart = time.time()    \n",
    "\n",
    "fileset = {\n",
    "    'DoubleMuon': [\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root',\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012C_DoubleMuParked.root',\n",
    "    ],\n",
    "    'ZZ to 4mu': [\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/ZZTo4mu.root'\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "run = processor.Runner(\n",
    "    executor = processor.FuturesExecutor(compression=None, workers=4),\n",
    "    schema=BaseSchema,\n",
    "    chunksize=100_000,\n",
    ")\n",
    "\n",
    "output = run(\n",
    "    fileset,\n",
    "    \"Events\",\n",
    "    processor_instance=FancyDimuonProcessor(),\n",
    ")\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nevt = output['cutflow']['ZZ to 4mu']['all events'] + output['cutflow']['DoubleMuon']['all events']\n",
    "print(\"Events/s:\", nevt / elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows is just us looking at the output, you can execute it if you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplhep\n",
    "# scale ZZ simulation to expected yield\n",
    "lumi = 11.6  # 1/fb\n",
    "zzxs = 7200 * 0.0336**2  # approximate 8 TeV ZZ(4mu)\n",
    "nzz = output['cutflow']['ZZ to 4mu']['all events']\n",
    "\n",
    "scaled = {}\n",
    "for name, h in output.items():\n",
    "    if isinstance(h, hist.Hist):\n",
    "        scaled[name] = h.copy()\n",
    "        scaled[name].scale({\"ZZ to 4mu\": lumi * zzxs / nzz}, \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "ax = hist.plot1d(scaled['nMuons'], overlay='dataset')\n",
    "ax.set_ylim(1, None)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hist.plot1d(scaled['mass'].rebin(\"mass\", 4), overlay='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hist.plot1d(scaled['mass_z1'], overlay='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hist.plot1d(scaled['mass_z2'], overlay='dataset')\n",
    "ax.set_xlim(2, 300)\n",
    "# ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hist.plot1d(scaled['pt_z1_mu1'], overlay='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hist.plot1d(scaled['pt_z1_mu2'], overlay='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
